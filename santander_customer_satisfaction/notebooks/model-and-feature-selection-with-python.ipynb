{
  "cells": [
    {
      "metadata": {
        "_uuid": "8436ee9884307044f40c0013288240699642141c",
        "_cell_guid": "fa7fdf8e-5998-437c-8473-ec2ef971bbf0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# THIS WAS MY FIRST SCRIPT AND IT HAS SOME FLAWS. \n# IT'S STILL HERE BECAUSE I GOT FOND OF IT.\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%pylab inline\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom itertools import combinations\nfrom numpy import array,array_equal\n\nfrom sklearn import cross_validation as cv\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import linear_model \nfrom sklearn import naive_bayes \n\nimport xgboost as xgb\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]\n).decode(\"utf8\"))\n\n\n# Any results you write to the current directory are saved as output.\ndef print_shapes():\n    print('Train: {}\\nTest: {}'.format(train_dataset.shape, test_dataset.shape))\ntrain_dataset = pd.read_csv('../input/train.csv', index_col='ID')\ntest_dataset = pd.read_csv('../input/test.csv', index_col='ID')\n\nprint_shapes()\n# How many nulls are there in the datasets?\nnulls_train = (train_dataset.isnull().sum()==1).sum()\nnulls_test = (test_dataset.isnull().sum()==1).sum()\nprint('There are {} nulls in TRAIN and {} nulls in TEST dataset.'.format(nulls_train, nulls_test))\n# Remove constant features\n\ndef identify_constant_features(dataframe):\n    count_uniques = dataframe.apply(lambda x: len(x.unique()))\n    constants = count_uniques[count_uniques == 1].index.tolist()\n    return constants\n\nconstant_features_train = set(identify_constant_features(train_dataset))\n\nprint('There were {} constant features in TRAIN dataset.'.format(\n        len(constant_features_train)))\n\n# Drop the constant features\ntrain_dataset.drop(constant_features_train, inplace=True, axis=1)\n\n\nprint_shapes()\n# Remove equals features\n\ndef identify_equal_features(dataframe):\n    features_to_compare = list(combinations(dataframe.columns.tolist(),2))\n    equal_features = []\n    for compare in features_to_compare:\n        is_equal = array_equal(dataframe[compare[0]],dataframe[compare[1]])\n        if is_equal:\n            equal_features.append(list(compare))\n    return equal_features\n\nequal_features_train = identify_equal_features(train_dataset)\n\nprint('There were {} pairs of equal features in TRAIN dataset.'.format(len(equal_features_train)))\n\n# Remove the second feature of each pair.\n\nfeatures_to_drop = array(equal_features_train)[:,1] \ntrain_dataset.drop(features_to_drop, axis=1, inplace=True)\n\nprint_shapes()\n# Define the variables model.\n\ny_name = 'TARGET'\nfeature_names = train_dataset.columns.tolist()\nfeature_names.remove(y_name)\n\nX = train_dataset[feature_names]\ny = train_dataset[y_name]\n\n# Save the features selected for later use.\npd.Series(feature_names).to_csv('features_selected_step1.csv', index=False)\nprint('Features selected\\n{}'.format(feature_names))\n   \n    \n# Proportion of classes\ny.value_counts()/len(y)\n\nskf = cv.StratifiedKFold(y, n_folds=3, shuffle=True)\nscore_metric = 'roc_auc'\nscores = {}\n\ndef score_model(model):\n    return cv.cross_val_score(model, X, y, cv=skf, scoring=score_metric)\n\n# time: 10s\nscores['tree'] = score_model(tree.DecisionTreeClassifier()) \n\n# time: 9s\nscores['extra_tree'] = score_model(ensemble.ExtraTreesClassifier())\n\n# time: 7s\nscores['forest'] = score_model(ensemble.RandomForestClassifier())\n\n# time: 33s\nscores['ada_boost'] = score_model(ensemble.AdaBoostClassifier())\n\n# time: 1min\nscores['bagging'] = score_model(ensemble.BaggingClassifier())\n\n# time: 2min30s\nscores['grad_boost'] = score_model(ensemble.GradientBoostingClassifier())\n\n# time: 49s\nscores['ridge'] = score_model(linear_model.RidgeClassifier())\n\n# time: 4s\nscores['passive'] = score_model(linear_model.PassiveAggressiveClassifier())\n\n# time: 4s\nscores['sgd'] = score_model(linear_model.SGDClassifier())\n\n# time: 3s\nscores['gaussian'] = score_model(naive_bayes.GaussianNB())\n\n# time: 4min\nscores['xgboost'] = score_model(xgb.XGBClassifier())\n\n\n# Print the scores\nmodel_scores = pd.DataFrame(scores).mean()\nmodel_scores.sort_values(ascending=False)\nmodel_scores.to_csv('model_scores.csv', index=False)\nprint('Model scores\\n{}'.format(model_scores))\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}